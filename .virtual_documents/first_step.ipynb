#import all the basic python libraries
import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt


#for timeseries RNN
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,SimpleRNN


#Import keras timegenerator and min max scaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.preprocessing import MinMaxScaler


#Create simple sine wave using numpy
x = np.linspace(0,50,768)
y = np.sin(x)
x


y



plt.plot(x,y)


#Load the data into pandas dataframe index as x , data as y
df = pd.DataFrame(data=y, index=x, columns=['sine'])
df.head(10)


len(df)


#testing data precentage 20% 
test_percent = 0.2


#Number of data points reserved for testing the model
len(df)*test_percent


#we write this line of code to find the test data length
test_length = np.round(len(df)*test_percent)


test_length


#the test data starts at this index
test_start_index = int(len(df) - test_length)


test_start_index


#create separate training and testing datasets 
#training data includes start to test_start_index
data_train = df.iloc[: test_start_index]
#testing data starts from test_start_index to end of the dataframe
data_train = df.iloc[test_start_index :]


data_train.head(5)


data_train = df.iloc[: test_start_index]
data_train = df.iloc[test_start_index :]
data_test = df.iloc[test_start_index :] 


data_train.head(5)


data_test.head(5)





#create a minmax scaler to normalize the data 
scaler = MinMaxScaler()


#train the scaler to perform the normalization
scaler.fit(data_train)


#normalize both the training and testing dataset
normalized_train = scaler.transform(data_train)
normalized_test = scaler.transform(data_test)


## Create a time series generator instance


#set the length of input sequence : the number of steps are used to predict the future
length = 50


#batch_Size tells about the number of time series sample in each batch 
batch_size = 1


# create a time series generator for training 
train_tsGenerator50 = TimeseriesGenerator(normalized_train, normalized_train, length=length, batch_size=batch_size)


len(normalized_train)


#run this command to check how does the batch look like
X,y = train_tsGenerator50[0]


#to get flatten array
X.flatten()


# predicts the next value
y





# we write this command to check howmany features have been used in the traning model
n_features=1
#deine model
model = Sequential()
# add 2 layers simple RNN and dense layer 
model.add(SimpleRNN(64,input_shape=(length,n_features)))
model.add(Dense(1))





#compile the model and use optimizer Adam and Loss MSE , here we use MSE because the data is continuos : aregression problem
model.compile(optimizer='adam', loss='mse')
model.summary()


#fit the model by using the below command 
model.fit_generator(train_tsGenerator50,epochs=5)





#load the loss data into a dataframe and visulaize the dataframe
df_model_loss=pd.DataFrame(model.history.history)
df_model_loss.plot()





#number of time steps of the input time series
length


#1st series  batch 
first_eval_batch = normalized_train[-length :]
first_eval_batch


#reshape the batch it is necessary to be in 3d
first_eval_batch = first_eval_batch.reshape((1,length,n_features))
first_eval_batch


#now check the shape
first_eval_batch.shape





# the full code is for evaluation, at first create a empty set to store all predicitons 
test_predictions = []
#this is last 50 datapoints of the train set 
first_eval_batch = normalized_train[-length:]
#reshape the batch to 3d array
current_batch = first_eval_batch.reshape((1,length, n_features))
#run for loopto make a prediction and get the value and store the prediction 
for i in range(len(data_test)):
    current_pred = model.predict(current_batch)[0]
    test_predictions.append(current_pred)
    # generate a new batch to prepare for the next iteration of the testing and drop the first data of the current inputsequence
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)


#convert the scaled results back to the real values 
true_predictions = scaler.inverse_transform(test_predictions)
true_predictions


data_test


#copy the true values of prediction into the dataframe of original data set
data_test['predictions'] = true_predictions


#updated data test 
data_test


#visualize the data of updated test data 
#compare the predicted sine wave against the original sine wave
data_test.plot(figsize=(12,8))
